---
title: "A brief introduction to text mining in R"
author: "Shelly Lachish"
date: "02/02/2021"
output: 
  html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
library(here)
library(tidyverse)
library(lubridate)
library(readtext)
library(quanteda)
library(quanteda.sentiment)
```

## Importing text data: package(readtext)

The **readtext** function from the package with the same name, detects the file formats of a given list of files and extracts the content into a data.frame. **readtext** supports .txt, .json, .csv, .tab, .tsv, .xml, .pdf, .doc, .docx, .odt, .rtf, files from URLs and archive file (.zip, .tar, .tar.gz, .tar.bz). It can read in multiple files at once and supports glob/wildcard expressions.

The parameter *docvarsfrom* allows you to set metadata variables by splitting on file name or path names.

```{r importing_text_data, include=TRUE, warning=FALSE, message=FALSE}
#Import text data files
#..with docvars taken from file paths (file paths must be same length)
extracted_texts<- readtext(here('Working_Data/import_data_types/*'), docvarsfrom = "filepaths", dvsep = "/")
print(extracted_texts)
print(extracted_texts[1], n = 12)

#Docvars from file names (File names need to be standardised across list - same num/type separator)
extracted_texts <- readtext(here('Working_Data/import_data_types/Mindy*'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "place", "year"))
print(extracted_texts, n=4)

# View beginning of the first extracted text
cat(substr(extracted_texts$text[1] , 0, 400))

#Get pdfs
RC_data <- readtext(here('Working_Data/import_data_types/*.pdf'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "journal", "year"))
RC_data
str(RC_data)
```

### A note on encodings

This is from the readtext package tutorial: "As encoding can also be a challenging issue for those reading in texts, we include functions for diagnosing encodings on a file-by-file basis, and allow you to specify vectorized input encodings to read in file types with individually set (and different) encodings. (All encoding functions are handled by the stringi package.)"

## Load our twitter data

Here we will use a data set of \~7,000 tweets with the subject "covid19" (obtained using the rtweet package from the 8th - 15th Feb) to understand concepts in text analysis, do some basic text exploration, run some simple statistics and perform a sentiment analysis on the content of the tweets

```{r load_tweet_data, message = FALSE}

#Load the data
covid_data<-read_tsv(here('Working_Data/covid19_tweets.tsv'))

#Clean the dataset
covid_data_clean <- covid_data %>% 
  #Remove @mentions
  mutate(text_clean = str_replace_all(text, '@\\S+', ' ')) %>%
  #Remove URLs
  mutate(text_clean = str_replace_all(text_clean, 'http\\S+', ' ')) %>%  
  #trim new line indicators and retweet indicators
  mutate(text_clean = str_replace_all(text_clean, '\\n|\\r', ' ')) %>%
  #Replace ampersand indicators
  mutate(text_clean = str_replace_all(text_clean, '&amp', 'and')) %>%
  #Remove unicode character indicators <U+0001F449> (used in place of emojis)
  mutate(text_clean = str_replace_all(text_clean, '<U.*>', ' ')) %>%
  #trim leading/tailing whitespace
  mutate(text_clean = trimws(text_clean, which = 'both')) %>%     
  #Format date to ymd
  mutate(tweet_date = as.Date(lubridate::ymd_hms(created_at))) %>% 
  #Add day of week variable
  mutate(week_day = lubridate::wday(ymd_hms(created_at), label = T)) %>%
  #Add UK vs US location variable
  mutate(location_US_UK = ifelse(str_detect(location, regex("uk|britain|england|scotland|wales", ignore_case = T)), "UK", 
                                 ifelse(str_detect(location, regex("us|usa|america*", ignore_case = T)), "US", NA))) %>%
  select (-text, -created_at)
```

## From data to corpus/corpora

```{r build_corpus}

#Create your corpus
covid_corpus<-corpus(covid_data_clean, text_field = 'text_clean')
head(docvars(covid_corpus))

#Inspecting the corpus
texts(covid_corpus)[2]
#This is what the original tweet (prior to string manipulation) looked like
covid_data$text[2]
summary(covid_corpus, n = 5)

#Can save and plot from this (but only summary of first 50 texts)
corp_summary <- summary(covid_corpus)
ggplot(data = corp_summary, aes(x = log(followers_count), y = Tokens, group = 1)) + geom_line() + geom_point()  + theme_bw()

```

The corpus can be manipulated in a variety of ways (analogous to a data frame). You can subset, concatenate, trim components from the text body, and change the unit of texts between documents, paragraphs and sentences. You can extract segments of texts and tags from documents (useful when you analyze sections of documents or transcripts separately). You can also perform a keywords-in-context search (kwic function).

```{r corpus_manipulations}
#Corpus manipulations
#Subset a corpus
corp_monday <- corpus_subset(covid_corpus, week_day == 'Mon')
ndoc(corp_monday)

#concatenate two corpus
corp_tuesday <- corpus_subset(covid_corpus, week_day == 'Tue')
ndoc(corp_tuesday)
new_corp <- corpus(corp_tuesday + corp_monday)
ndoc(new_corp)

#change unit length to sentences/paragraphs/ or document (default)
corp_sentences <- corpus_reshape(corp_monday, to = "sentences")
ndoc(corp_sentences)
summary(corp_sentences, n = 5)

#The kwic function (keywords-in-context) performs a search for a word and allows us to view the contexts in which it occurs:
head(kwic(covid_corpus, pattern = 'vaccin*'))
head(kwic(covid_corpus, pattern = phrase("conspiracy theor*")))
texts(covid_corpus)[1434]
texts(covid_corpus)[2365]

```

## Tokenisation

The function `tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries. Usually a corpus is passed to `tokens()`, but it works with a character string too. By default, `tokens()` only removes separators (typically white spaces), but you can set other separators. You can (should?) remove punctuation and numbers prior to tokenisation.

You can remove tokens that you are not interested in using `tokens_select()`. Usually we remove grammatical words that have little or no substantive meaning in pre-processing. These are known as **stopwords**. The function `stopwords()` returns a pre-defined list of function words.

```{r get_tokens}
#Get Tokens

#Describe a bit how tokens are interpreted
txt <- c(text1 = 'This is $10 in 999 different ways,\n up and down; left and #right!', 
         text2 = '@shellstar working: @ something on #Rtexttutorial 2day\t4ever !')#
tokens(txt)
tokens(txt, remove_punct = TRUE, remove_symbols = T, remove_numbers = T)

#----------------------------------------
#Get tokens from covid_tweets data
#----------------------------------------
#Remove punctuation, symbols, numbers
head(covid_toks<-tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T))

#Remove stopwords
head(covid_toks_nostop <- tokens_remove(covid_toks, pattern = stopwords("english")))

#Select or remove particular interesting words
head(covid_toks_vaccination <- tokens_select(covid_toks, pattern = c("vaccin*", "immunis*")))
head(covid_toks_vaccination <- tokens_select(covid_toks, pattern = c("vaccin*", "immunis*"), selection = 'remove'))
```

To preserve [multiword expressions]{.ul} (which you may want to do in a bag-of-words type analysis), you have to compound them using `tokens_compound()`. For example, `tokens_compound(covid_toks, pattern = phrase(c('Boris Johnson','hospital admission*')))`. You can discover multiword expressions in your tokens using `textstat_collocations()`. This kind of thing is also useful if you want to create ngrams - particularly useful for looking for negative bigrams: `toks_neg_bigram<-tokens_compound(toks, pattern = phrase("not *"))` and then once you've found them you can select them from your corpus: `toks_neg_bigram_select <- tokens_select(toks_neg_bigram, pattern = phrase("not_*"))`

## From tokens to dfm (document feature matrix, aka document term matrix)

The function `dfm()` constructs a document-feature matrix (DFM) [from a tokens object or a corpus object]{.ul} (in which case the tokenisation occurs internally). `topfeatures()` will list the features (terms/tokens) in descending order. `textstat_frequency()` shows both term and document frequencies. You can also use the function to find the most frequent features within groups.

```{r get_dfm}
covid_toks <- tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T)

my_stopwords<-c('covid*', 'corona*', '#covid*', '#corona*')

dfmat_covidtweets <- dfm(covid_toks, tolower = TRUE, remove = c(stopwords(), my_stopwords, stem = FALSE))

dfmat_covidtweets
docvars(dfmat_covidtweets)
  ndoc(dfmat_covidtweets)
  nfeat(dfmat_covidtweets)
  head(docnames(dfmat_covidtweets))
  head(featnames(dfmat_covidtweets))
  head(rowSums(dfmat_covidtweets), 10)
  head(colSums(dfmat_covidtweets), 10)

topfeatures(dfmat_covidtweets, 10)

#For analysis purposes it is often a really good idea to remove very sparse terms from the dfm
(dfmat_covidtweets<-dfmat_covidtweets %>% 
                    dfm_trim(min_termfreq = 10, verbose = FALSE))
```

As with `tokens()`, you can select features from a DFM using `dfm_select()`. You can also select features based on the length of features (e.g. keep features consisting of at least five characters: `dfm_keep(dfmat_inaug, min_nchar = 5)`). While `dfm_select()` selects features based on patterns, `dfm_trim()` does this based on feature frequencies. If `min_termfreq = 10`, features that occur less than 10 times in the corpus are removed. If you want to convert the frequency count to a proportion within documents, use `dfm_weight(scheme = "prop")`

## Simple text analysis of the tweets

```{r text_exploration}

tstat_freq <- textstat_frequency(dfmat_covidtweets)
head(tstat_freq, 5)
tail(tstat_freq, 5)

dfmat_covidtweets %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

#Let's take a look at a word cloud
set.seed(132)
textplot_wordcloud(dfmat_covidtweets, max_words = 100)

##SHOULD WE STEM TERMS? - we have 'vaccine' and 'vaccines' and 'vaccination' as top terms
(dfmat_covidtweets_stem <- dfm(covid_toks, tolower = TRUE, remove = c(stopwords(), my_stopwords), stem = TRUE) %>%
                              dfm_trim(min_termfreq = 10, verbose = FALSE))

dfmat_covidtweets_stem %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

textplot_wordcloud(dfmat_covidtweets_stem, max_words = 100)
```

## Differences between groups 

```{r test_grouping_analysis}
#----------------------
#Look at grouping variables 
#----------------------
dfmat_covidtweets_gpd<-dfm_group(dfmat_covidtweets_stem, groups = 'location_US_UK')
tstat_freq_gpd <- textstat_frequency(dfmat_covidtweets_stem, n = 30, groups = 'location_US_UK')

#Plot group differences
ggplot(data = tstat_freq_gpd, aes(x = factor(nrow(tstat_freq_gpd):1), y = frequency)) +
    geom_point() +
    facet_wrap(~ group, scales = "free") +
    coord_flip() +
    scale_x_discrete(breaks = nrow(tstat_freq_gpd):1,labels = tstat_freq_gpd$feature) +
    labs(x = NULL, y = "Relative frequency")


#Let's subset the dfm to look at popular hastags
dfmat_hashtags <- dfm_select(dfmat_covidtweets, pattern = '#*') 
textplot_wordcloud(dfmat_hashtags, max_words = 100)
textplot_wordcloud(dfm_select(dfmat_covidtweets_gpd, pattern = '#*'), comparison = TRUE, max_words = 100, color = c("blue", "red"))
```

## Sentiment analysis of the twitter data

The aim of sentiment analysis is to determine the polarity of a text (i.e., whether the emotions expressed in it are rather positive or negative). This is often done by word lists and by counting terms that were previously assigned to the categories *positive* or *negative* (sometimes a third category is included; *neutral,* and sometimes words can also be assigned a sentiment strength). In some lexicons, words may also be assigned to emotions like joy, anger, sadness, and so forth.

So basically a dictionary ('sentiment/topic dictionary') is used to group a number of individual terms into a 'sentiment' category, then the content of the whole text as the sum of the sentiment content of the individual words. (This isn't the only way to approach sentiment analysis, but it is an often-used approach). The quanteda package provides access to four general-purpose sentiment dictionaries (for English).

We will apply the the Lexicoder Sentiment Dictionary to the selected contexts using **textstat_polarity()**. All three of these lexicons are based on unigrams (i.e., single words), and some bigrams.

```{r sentiment_analysis, message=F}
#Dictionary based sentiment analysis
lengths(data_dictionary_LSD2015)
print(data_dictionary_LSD2015, max_nval = 20)

#Can also make and use your own bespoke dictionary
my_dictionary <- dictionary(list(pos = c("happiness", "joy", "light"), neg = c("sadness", "anger", "darkness")))

#Set the polarity categories we wish to use
my_polarity <- list(pos = c("positive", "neg_negative"), neg = c("negative", "neg_positive"))
polarity(data_dictionary_LSD2015) <- my_polarity

#Get sentiment scores on our original corpus 
sentiment<-textstat_polarity(dfmat_covidtweets, dictionary = data_dictionary_LSD2015) #default function == log(\frac{pos}{neg})
#There are other functions available - or you can write your own and feed it in using the option 'fun = xxx'
#If you have very long documents - you may want to weight the sentiment values by the length (total words) in the document. 
range(sentiment$sentiment)
head(sentiment)

#Alternative Method
#use function tokens_lookup to assign sentiment to tokens
covidtweets_lsd <- tokens_lookup(covid_toks, dictionary = data_dictionary_LSD2015)
# if you do the above - then need to create a document document-feature matrix
dfmat_covidtweets_lsd <- dfm(covidtweets_lsd) %>% dfm_group(c('location_US_UK', 'week_day')) 

#Create data for plotting
sentiment_dat<- cbind(sentiment, docvars(covid_corpus)) %>%
  filter(!is.na(location_US_UK)) %>% 
  mutate(week_day = fct_relevel(week_day, 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')) %>% 
  group_by(location_US_UK, week_day) %>% 
  summarise(mean_sent = mean(sentiment))
 
ggplot(data = sentiment_dat, aes(y = mean_sent, x = week_day, group = location_US_UK, color = location_US_UK))+
  geom_line()+
  geom_hline(yintercept = 0)+
  theme_bw()

#Specific Sentiments - Bing Liu Sentiment Lexicon
data(sentiments, package = "tidytext")
sentiment<-textstat_polarity(dfmat_covidtweets, lexicon == "bing")
as.dictionary(subset(sentiments, lexicon == "bing"))

sentiment<-textstat_polarity(dfmat_covidtweets, dictionary = data_dictionary_LSD2015)

```

## Other Dictionaries

```{r bin_dictionary}
#Sentiment "type" dictionary
lengths(data_dictionary_NRC)
print(data_dictionary_NRC, max_nval = 10)

#Can also make and use your own bespoke dictionary
my_dictionary <- dictionary(list(pos = c("happiness", "joy", "light"), neg = c("sadness", "anger", "darkness")))

#Set the polarity categories we wish to use
my_new_polarity <- list(pos = c("positive"), neg = c("negative"))
polarity(data_dictionary_NRC) <- my_new_polarity

#Alternative Method - use function tokens_lookup to assign sentiment to tokens
covidtweets_new_lsd <- tokens_lookup(covid_toks, dictionary = data_dictionary_NRC, fun = )
# if you do the above - then need to create a document document-feature matrix
dfmat_sentiment_gpd <- dfm(covidtweets_new_lsd) %>% dfm_group(c('location_US_UK'))
dfm_sentiment_prop <- dfm_weight(dfmat_sentiment_gpd, scheme = "prop")

plot_dat <- dfm_sentiment_prop %>% 
  convert(., to ="data.frame") %>% 
  pivot_longer(!doc_id, names_to = "sentiment_types", values_to = "proportions")

ggplot(data = plot_dat, aes(x=sentiment_types, y=proportions, fill=doc_id)) + 
  geom_bar(stat="identity", position=position_dodge())
```

## Case Study: Using supervised machine learning to build a text classifier

**Aim:** create a automatic/semiautomatic classification system that could code (label) the free-text answers into one of the existing coded categories.

**Process:**

1.  Create the corpus

    -   I have done the cleaning/pre-processing of the free-text field already

2.  Build the DFM

3.  Split the data into training and testing datasets

4.  Apply a linear SVM classifier model to classify and predict (can use other models if needed)

5.  Assess the accuracy/precision of our model

6.  Use the model to 'label' the new data

```{r supervised_ML_example}
#load the text mining, ML classification packages
library(quanteda.textmodels)
library(caret)

#*************************************************************
# 1. Read in data for building the ML model ====

coded_freetext<-read_tsv(here('Working_Data/Supervised_ML/labelled_dataset.txt'))
dim(coded_freetext)


  #table(coded_freetext$new_meaning)
  #table(coded_freetext$new_code)

#*************************************************************
# 4. Create the corpus ====

freetext_codesandtext <- coded_freetext_clean %>% 
  select(doc_id, new_code, freetext_clean) %>% 
  rename(Label = new_code) 

##create the corpus and clean
freetext_corpus<-corpus(freetext_codesandtext, text_field = 'freetext_clean') 

#add the doc_id as to docvars
docvars(freetext_corpus, field = 'doc_id') <- freetext_codesandtext$doc_id

  #list the docvars [Label is a docvars]
  #head(docvars(freetext_corpus) )

  #see bits of the corpus:
  #summary(freetext_corpus, n = 20)
  #texts(freetext_corpus)[9]

#*************************************************************
# 5. Build the Document Feature Matrix ====

#EXTRA STOPWORDS to remove from corpus
#This lists all the abbreviation variations for participant to remove from the corpus and other random words found during data clean
#Build the Document Feature Matrix from text field
base_dfm <- dfm(freetext_corpus, verbose = T, remove_punct = T, remove_numbers = T, remove_symbols = T, stem = T,
                           remove = stopwords('english')) 

#remove low freq words and terms only found in a few documents
base_dfm_nolows <-dfm_trim(base_dfm, min_termfreq = 6, min_docfreq = 3)

  #dim(freetext_corpus_dfm_nolows)
  #topfeatures(freetext_corpus_dfm_nolows, 20)
  #featnames(freetext_corpus_dfm_nolows)
  #sparsity(freetext_corpus_dfm_nolows) #0.9973569


#*************************************************************
# 6. Split the data into training and testing datasets ====

#Split in two parts 60:40. A 60% that will become our training data set / 40% for our testing data set 

#set the seed so random draws are reproducible
set.seed(4321)

training_data <- dfm_sample(base_dfm_nolows, size = 0.6*nrow(base_dfm_nolows))
testing_data<-base_dfm_nolows[setdiff(docnames(base_dfm_nolows), docnames(training_data)), ]

```

```{r}
#*************************************************************
#NOTES -->
  #Initial investigation showed that 60:40 was the best split (no improvement with higher % for training)
  #Initial investigation showed that stemming words did not improve accuracy or kappa significantly
  #Initial investigation showed that weighting the dfm (i.e. using tf-idf matrix) did not improve accuracy or kappa significantly
  #Initial investigation showed that svm model outperforms the nb model
  #If you need to do stemming then run this line of code
      #>  stemd_dfm <- dfm(freetext_corpus, verbose = T, remove_punct = T, remove_numbers = T, remove_symbols = T, remove = c(stopwords('english'), mystopwords), stem = T) 
  #If you want to use tf-idf weighting then run these lines of code
      #> base_dfm_nolows <-dfm_trim(base_dfm, min_termfreq = 6, min_docfreq = 3)
      #> based_dfm_tfidf <-dfm_tfidf(base_dfm_nolows, scheme_tf = "count", scheme_df = "inverse")


#*************************************************************
#APPLY Linear SVM classifier model to classify and predict ====

#Run the model
svm_model <- textmodel_svm(training_data, training_data$Label)
  #summary(svm_model)

#Get the predictions (force = T means to only use features that exist in the both training and prediction dfms)
svm_predicted_class <- predict(svm_model, newdata = testing_data, type = "class", force = T)
svm_actual_class <- testing_data$Label
svm_tab_class <- table(svm_actual_class, svm_predicted_class); svm_tab_class
sum(diag(svm_tab_class))/dim(testing_data)[1] # 0.914

#Look at accuracy etc
confusionMatrix(svm_tab_class, mode = "everything")


#***********************************************************************
# 8. APPLY SVM model to predict classes of new freetext data fields ====

#load the new data
newfields_data<-fread(here('Data/preprocessed_newfields_data.txt'))

#create the corpus and clean
newfields_corpus<-corpus(newfields_data, text_field = 'freetext_clean') 

#add the doc_id as to docvars
docvars(newfields_corpus, field = 'doc_id') <- newfields_data$doc_id

#Add new stopwords (these are common words that I found in this second corpus that were not in the first))
extra_stopwords<-c('core', 'core protocol', 'core scan', 'scans', 'only', 'due to', 'fmri', 'history', 'trigger')

newdata_dfm <- dfm(newfields_corpus, verbose = T, remove_punct = T, remove_numbers = T, remove_symbols = T, stem = T,
                            remove = c(stopwords('english'), mystopwords)) 

#remove low freq words and terms only found in a few documents
newfields_dfm_nolows <-dfm_trim(newdata_dfm, min_termfreq = 6, min_docfreq = 3)


#Match the features of the new corpus to that used to build the model
newfields_matched <- dfm_match(newfields_dfm_nolows, features = featnames(training_data))

#Predict new classes for the new fields
newfields_predicted_class <- predict(svm_model, newdata = newfields_matched, type = "class")
newfields_predicted_probs <- predict(svm_model, newdata = newfields_matched, type = "probability")

#Find second highest class
#secondhighest<- newfields_predicted_probs %>%
#  as_tibble() %>% 
#  mutate(doc_id  = newfields_data$doc_id) %>% 
#  pivot_longer(!doc_id, names_to = "PredLabel", values_to = "Probs") %>% 
#  group_by(doc_id) %>%
#  arrange(doc_id, desc(Probs))%>% 
#  slice(2) %>%  #This takes the second value for each group
#  select(doc_id, PredLabel)

#bind the classifications to the original data
model_classified_newfields<-cbind(newfields_data, newfields_predicted_class) %>% 
  rename(ModelClassified_Label = newfields_predicted_class)

```

```{r spell_check_textfields}
#*************************************************************
library(hunspell)

# Get bad words.
  incorrectly_spelled_words <- hunspell(coded_freetext_clean$freetext_clean) %>% unlist()

# Extract the first suggestion for each bad word. - TAKES ~ A FEW  MINUTES TO RUN
  suggestions <- sapply(incorrectly_spelled_words, function(x) hunspell_suggest(x)[[1]][1]) %>% unname()

#replace in the original text - TAKES ~ A FEW  MINUTES TO RUN
  corrected_coded_freetext_clean <- mutate(coded_freetext_clean, 
                             freetext_clean = stringi::stri_replace_all_regex(str = freetext_clean, 
                                              pattern = paste0('\\b', incorrectly_spelled_words, '\\b'),
                                              replacement = suggestions, vectorize_all = FALSE))


```
