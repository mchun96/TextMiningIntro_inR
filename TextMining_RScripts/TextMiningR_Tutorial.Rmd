---
title: "A brief introduction to text mining in R"
author: "Shelly Lachish"
date: "02/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
library(tidyverse)
library(data.table)
library(here)
library(readtext)
library(quanteda)
library(lubridate)
```

## Importing text data: package(readtext)

The **readtext** function from the package with the same name, detects the file formats of a given list of files and extracts the content into a data.frame. **readtext** supports .txt, .json, .csv, .tab, .tsv, .xml, .pdf, .doc, .docx, .odt, .rtf, files from URLs and archive file (.zip, .tar, .tar.gz, .tar.bz). It can read in multiple files at once and supports glob/wildcard expressions. The parameter *docvarsfrom* allows you to set metadata variables by splitting on file name or path names.

```{r importing_text_data, include=TRUE}
#Import text data files
#..with docvars taken from file paths (file paths must be same length)
extracted_texts<- readtext(here('Working_Data/import_data_types/*'), docvarsfrom = "filepaths", dvsep = "/")
print(extracted_texts)
print(extracted_texts[1], n = 12)

#Docvars from file names (File names need to be standardised across list - same num/type separator)
extracted_texts <- readtext(here('Working_Data/import_data_types/Mindy*'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "place", "year"))
print(extracted_texts, n=4)

# View beginning of the first extracted text
cat(substr(extracted_texts$text[1] , 0, 400))
```

```{r get_pdfs, include=T}
#Load only pdfs from a folder
RC_data <- readtext(here('Working_Data/import_data_types/*.pdf'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "journal", "year"))
RC_data
str(RC_data)
```

### A note on encodings

This is from the readtext package tutorial: "As encoding can also be a challenging issue for those reading in texts, we include functions for diagnosing encodings on a file-by-file basis, and allow you to specify vectorized input encodings to read in file types with individually set (and different) encodings. (All encoding functions are handled by the stringi package.)"

## From data to corpus (corpora)

Here we will use a data set of \~XXXX tweets with the subject "covid19" (obtained using the rtweet package from the 8th - 15th Feb) to understand concepts in text analysis and perform a sentiment analysis on the content of the tweets

```{r build_corpus_tweets, include=TRUE}

#Load the data
covid_data<-readtext(here('Working_Data/covid19_tweets.tsv'), text_field = "text")

#Clean the dataset
covid_data_clean <- covid_data %>% 
  #Remove hashtags
  mutate(text_clean = str_replace_all(text, '#', '')) %>% 
  #Remove @mentions
  mutate(text_clean = str_replace_all(text_clean, '@\\S+', '')) %>%
  #Remove URLs
  mutate(text_clean = str_replace_all(text_clean, 'http\\S+', '')) %>%  
  #trim literal new line indicators
  mutate(text_clean = str_replace_all(text_clean, '\\n', '')) %>%
  #set all words to lower case
  mutate(text_clean = tolower(text_clean)) %>% 
  #trim leading/tailing whitespace
  mutate(text_clean = trimws(text_clean, which = 'both')) %>%     
  #Format date to ymd
  mutate(tweet_date = as.Date(lubridate::ymd_hms(created_at))) %>% 
  #Add day of week variable
  mutate(week_day = lubridate::wday(ymd_hms(created_at), label = T)) %>%
  select (-text, -created_at)

#Create your corpus
covid_corpus<-corpus(covid_data_clean, text_field = 'text_clean')
head(docvars(covid_corpus))

#Inspecting the corpus
texts(covid_corpus)[2]
summary(covid_corpus)

```

The corpus can be manipulated in a variety of ways (somewhat analogous to a data frame). You can subset, concatenate, trim components from the text body, and change the unit of texts between documents, paragraphs and sentences. You can extract segments of texts and tags from documents (useful when you analyze sections of documents or transcripts separately). You can also perform a keywords-in-context search (kwic function).

```{r corpus_manipulations}
#Corpus manipulations
#Subset a corpus
corp_monday <- corpus_subset(covid_corpus, week_day == 'Mon')
ndoc(corp_sunday)

#concatenate two corpus
corp_tuesday <- corpus_subset(covid_corpus, week_day == 'Tues')
new_corp <- corpus(corp_tuesday + corp_monday)
ndoc(new_corp)

#change unit length to sentences/paragraphs/ or token(default)
corp_sentences <- corpus_reshape(corp_monday, to = "sentences")
ndoc(corp_sentences)

#The kwic function (keywords-in-context) performs a search for a word and allows us to view the contexts in which it occurs:
head(kwic(covid_corpus, pattern = 'vaccination'))
head(kwic(covid_corpus, pattern = phrase("conspiracy theories")))
texts(covid_corpus)[345]
texts(covid_corpus)[898]
```

## Tokenisation and simple lexical analysis 

```{r get_tokens}
#Get Tokens
tokeninfo <- summary(covid_corpus)
if (require(ggplot2)) ggplot(data = tokeninfo, aes(x = location, y = Tokens, group = 1)) + 
    geom_line() + geom_point()  + theme_bw()

tokeninfo[which.max(tokeninfo$Tokens), ]




#Describe a bit how tokens are interpreted
#txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and #right!", 
#    text2 = "@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com#?page=123.")#
#tokens(txt)

```

## sentiment analysis of twitter data

```{r tweet_analysis_1, echo=FALSE}
#Load the data
covid_data<-readtext(here('Working_Data/covid19_tweets.tsv'), text_field = "text")

#Clean the dataset
covid_data_clean <- covid_data %>% 
  #Remove hashtags
  mutate(text_clean = str_replace_all(text, '#', '')) %>% 
  #Remove @mentions
  mutate(text_clean = str_replace_all(text_clean, '@\\S+', '')) %>%
  #Remove URLs
  mutate(text_clean = str_replace_all(text_clean, 'http\\S+', '')) %>%
  #set all words to lower case
  mutate(text_clean = tolower(text_clean)) %>% 
  #trim leading/tailing whitespace
  mutate(text_clean = trimws(text_clean, which = 'both')) %>%     
  #Format date to ymd
  mutate(tweet_date = as.Date(lubridate::ymd_hms(created_at))) %>% 
  #Add day of week variable
  mutate(week_day = lubridate::wday(ymd_hms(created_at), label = T)) %>% 
  select (-text, -created_at)

#Create your corpus
covid_corpus<-corpus(covid_data_clean, text_field = 'text_clean')
head(docvars(covid_corpus))

#Text Pre-processing
#remove punctuation
#remove numbers
#remove stopwords

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
