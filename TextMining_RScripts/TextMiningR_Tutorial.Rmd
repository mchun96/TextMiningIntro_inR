---
title: "A brief introduction to text mining in R"
author: "Shelly Lachish"
date: "15/03/2021"
output: 
  html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message = FALSE, warning = FALSE)
library(here)
library(tidyverse)
library(lubridate)
library(readtext)
library(quanteda)
library(quanteda.sentiment)
```

## STEP 1: Importing text data: package(readtext)

The **readtext** function from the package with the same name, detects the file formats of a given list of files and extracts the content into a data.frame. **readtext** supports .txt, .json, .csv, .tab, .tsv, .xml, .pdf, .doc, .docx, .odt, .rtf, files from URLs and archive file (.zip, .tar, .tar.gz, .tar.bz). It can read in multiple files at once and supports glob/wildcard expressions.

The parameter *docvarsfrom* allows you to set metadata variables by splitting on file name or path names.

```{r importing_text_data}
#Import text data files
#..with docvars taken from file paths (file paths must be same length)
list.files(here('Working_Data/import_data_types/'))
extracted_texts<- readtext(here('Working_Data/import_data_types/*'), docvarsfrom = "filepaths", dvsep = "/")
print(extracted_texts)
print(extracted_texts[1], n = 12)
print(extracted_texts$text[1:2])

#Docvars from file names (File names need to be standardised across list - same num/type separator)
extracted_texts <- readtext(here('Working_Data/import_data_types/Mindy*'), 
                            docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "place", "year"))
print(extracted_texts, n=4)
# View beginning of the first extracted text
cat(substr(extracted_texts$text[1] , 0, 400))

#Get pdfs
extracted_texts <- readtext(here('Working_Data/import_data_types/*.pdf'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "journal", "year"))
extracted_texts
str(extracted_texts)
summary(extracted_texts)
```

A note on [**file encodings**]{.ul}: "*As encoding can also be a challenging issue for those reading in texts, we include functions for diagnosing encodings on a file-by-file basis, and allow you to specify vectorized input encodings to read in file types with individually set (and different) encodings. (All encoding functions are handled by the stringi package.)*"

## Load our twitter data

Here we will use a data set of \~7,000 tweets with the subject "covid19" (obtained using the rtweet package from the 8th - 15th Feb) to:

-   understand concepts in text analysis (corpus, tokenisation, document feature matrix)

-   do some basic text exploration (word frequencies, wordclouds, keywords in context)

-   perform a sentiment analysis on the content of the tweets

```{r load_tweet_data, message = FALSE}

#Load the data
covid_data<-read_tsv(here('Working_Data/covid19_tweets.tsv'))

#Clean the dataset
covid_data_clean <- covid_data %>% 
  #Remove @mentions
  mutate(text_clean = str_replace_all(text, '@\\S+', ' ')) %>%
  #Remove URLs
  mutate(text_clean = str_replace_all(text_clean, 'http\\S+', ' ')) %>%  
  #trim new line indicators and retweet indicators
  mutate(text_clean = str_replace_all(text_clean, '\\n|\\r', ' ')) %>%
  #Replace ampersand indicators
  mutate(text_clean = str_replace_all(text_clean, '&amp', 'and')) %>%
  #Remove unicode character indicators <U+0001F449> (used in place of emojis)
  mutate(text_clean = str_replace_all(text_clean, '<U.*>', ' ')) %>%
  #trim leading/tailing whitespace
  mutate(text_clean = trimws(text_clean, which = 'both')) %>%     
  #Format date to ymd
  mutate(tweet_date = as.Date(lubridate::ymd_hms(created_at))) %>% 
  #Add day of week variable
  mutate(week_day = lubridate::wday(ymd_hms(created_at), label = T)) %>%
  #Add UK vs US location variable
  mutate(location_US_UK = ifelse(str_detect(location, regex("uk|britain|england|scotland|wales", ignore_case = T)), "UK", 
                                 ifelse(str_detect(location, regex("us|usa|america*", ignore_case = T)), "US", NA))) %>%
  select (-text, -created_at)
```

##### *ðŸ“– go to slides*

## STEP 2: From data to corpus

```{r build_corpus}

#Create your corpus
covid_corpus<-corpus(covid_data_clean, text_field = 'text_clean')
head(docvars(covid_corpus))

#Inspecting the corpus
texts(covid_corpus)[2]
#This is what the original tweet (prior to string manipulation) looked like
covid_data$text[2]
summary(covid_corpus, n = 5)

#Can save and plot from this (but only summary of first 'n' texts)
corp_summary <- summary(covid_corpus, n = 50)
ggplot(data = corp_summary, aes(x = log(followers_count), y = Tokens, group = 1)) + geom_line() + geom_point()  + theme_bw()
```

The corpus can be manipulated in a variety of ways (analogous to a data frame). You can subset, concatenate, trim components from the text body, and change the unit of texts between documents, paragraphs and sentences. You can extract segments of texts and tags from documents (useful when you analyze sections of documents or transcripts separately). You can also perform a keywords-in-context search (kwic function).

```{r corpus_manipulations}
#Corpus manipulations
#Subset a corpus
corp_monday <- corpus_subset(covid_corpus, week_day == 'Mon')
ndoc(corp_monday)

#concatenate two corpus
corp_tuesday <- corpus_subset(covid_corpus, week_day == 'Tue')
ndoc(corp_tuesday)
new_corp <- corpus(corp_tuesday + corp_monday)
ndoc(new_corp)

#change unit length to sentences/paragraphs/ or document (default)
corp_sentences <- corpus_reshape(corp_monday, to = "sentences")
ndoc(corp_sentences)
summary(corp_sentences, n = 7)

#The kwic function (keywords-in-context) performs a search for a word and allows us to view the contexts in which it occurs:
head(kwic(covid_corpus, pattern = 'vaccin*'))
head(kwic(covid_corpus, pattern = phrase("conspiracy theor*")))
texts(covid_corpus)[1649]
texts(covid_corpus)[2365]
```

##### *ðŸ“– go to slides*

## STEP 3: Tokenisation

The function `tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries. Usually a corpus is passed to `tokens()`, but it works with a character string too. By default, `tokens()` only removes separators (typically white spaces), but you can set other separators. You can (should?) remove punctuation and numbers prior to tokenisation.

You can remove tokens that you are not interested in using `tokens_select()`. Usually we remove grammatical words that have little or no substantive meaning in pre-processing. These are known as **stopwords**. The function `stopwords()` returns a pre-defined list of function words.

```{r get_tokens}
#Get Tokens

#Describe a bit how tokens are interpreted
txt <- c(text1 = 'This is $10 in 999 different ways,\n up and down; left and #right!', 
         text2 = '@shellstar working: @ something on #Rtexttutorial 2day\t4ever !')#
(tokens<-tokens(txt))
(tokens<-tokens(txt, remove_punct = TRUE, remove_symbols = T, remove_numbers = T))
ntoken(tokens)
ntype(tokens)

#----------------------------------------
#Get tokens from covid_tweets data
#----------------------------------------
#Remove punctuation, symbols, numbers
head(covid_toks<-tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T))

#Remove stopwords
head(covid_toks_nostop <- tokens_remove(covid_toks, pattern = stopwords("english")))

#Select or remove particular interesting words
head(covid_toks_vaccination <- tokens_select(covid_toks, pattern = c("vaccin*", "immunis*")))
head(covid_toks_vaccination <- tokens_select(covid_toks, pattern = c("vaccin*", "immunis*"), selection = 'remove'))
```

To preserve [multiword expressions]{.ul} (which you may want to do in a bag-of-words type analysis), you have to compound them using `tokens_compound()`. For example, `tokens_compound(covid_toks, pattern = phrase(c('Boris Johnson','hospital admission*')))`.

You can discover multiword expressions in your tokens using `textstat_collocations()`. This kind of thing is also useful if you want to create ngrams - particularly useful for looking for negative bigrams: `toks_neg_bigram<-tokens_compound(toks, pattern = phrase("not *"))` and then once you've found them you can select them from your corpus: `toks_neg_bigram_select <- tokens_select(toks_neg_bigram, pattern = phrase("not_*"))`

##### *ðŸ“– go to slides*

## STEP 4: Build the dfm (document feature matrix)

The function `dfm()` constructs a document-feature matrix (DFM) [from a tokens object or a corpus object]{.ul} (in which case the tokenisation occurs internally). `topfeatures()` will list the features (terms/tokens) in descending order. `textstat_frequency()` shows both term and document frequencies. You can also use the function to find the most frequent features within groups.

```{r get_dfm}
covid_toks <- tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T)

#Remove stopwords and bespoke list of non-informative words
my_stopwords<-c('covid*', 'corona*', '#covid*', '#corona*', '#sars*')

dfmat_covidtweets <- dfm(covid_toks, tolower = FALSE, remove = c(stopwords(), my_stopwords, stem = FALSE))
#Can build this straight from the corpus without first generating tokens
#dfmat_covidtweets <- dfm(covid_corpus, tolower = FALSE, remove = c(stopwords(), my_stopwords, stem = FALSE))

dfmat_covidtweets
docvars(dfmat_covidtweets)
  ndoc(dfmat_covidtweets)
  nfeat(dfmat_covidtweets)
  head(docnames(dfmat_covidtweets))
  head(featnames(dfmat_covidtweets))
  head(rowSums(dfmat_covidtweets), 10)
  head(colSums(dfmat_covidtweets), 10)

topfeatures(dfmat_covidtweets, 10)

#For analysis purposes it is often a really good idea to remove very sparse terms from the dfm
(dfmat_covidtweets<-dfmat_covidtweets %>% 
                    dfm_trim(min_termfreq = 10, verbose = FALSE))
```

As with `tokens()`, you can select features from a DFM using `dfm_select()`. You can also select features based on the length of features (e.g. keep features consisting of at least five characters: `dfm_keep(dfmat_inaug, min_nchar = 5)`). While `dfm_select()` selects features based on patterns, `dfm_trim()` does this based on feature frequencies. If `min_termfreq = 10`, features that occur less than 10 times in the corpus are removed. If you want to convert the frequency count to a proportion within documents, use `dfm_weight(scheme = "prop")`

## Step 5: Exploration/analysis

```{r text_exploration}

tstat_freq <- textstat_frequency(dfmat_covidtweets)
head(tstat_freq, 5)
tail(tstat_freq, 5)

dfmat_covidtweets %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

#Let's take a look at a word cloud
set.seed(132)
textplot_wordcloud(dfmat_covidtweets, max_words = 100)
```

##### SHOULD WE STEM TERMS? 

We have 'vaccine' and 'vaccines' and 'vaccination' as top terms - but these are all essentially the same word.

ðŸ“– go to slides

```{r text_exploration_2}
(dfmat_covidtweets_stem <- dfm(covid_toks, tolower = FALSE, remove = c(stopwords(), my_stopwords), stem = TRUE) %>%
                              dfm_trim(min_termfreq = 10, verbose = FALSE))


dfmat_covidtweets_stem %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

textplot_wordcloud(dfmat_covidtweets_stem, max_words = 100)
```

## Differences between groups

```{r test_grouping_analysis}
#----------------------
#Look at grouping variables 
#----------------------
dfmat_covidtweets_gpd<-dfm_group(dfmat_covidtweets_stem, groups = 'location_US_UK')
####dfmat_covidtweets_gpd<-dfm_weight(dfmat_covidtweets_gpd, scheme = 'prop')

#Plot group differences
gpd_diffs<- dfmat_covidtweets_gpd %>% textstat_frequency(n = 30, groups = 'location_US_UK')

ggplot(data = gpd_diffs, aes(x = factor(nrow(gpd_diffs):1), y = frequency)) +
    geom_point() +
    facet_wrap(~ group, scales = "free") +
    coord_flip() +
    scale_x_discrete(breaks = nrow(gpd_diffs):1,labels = gpd_diffs$feature) +
    labs(x = NULL, y = "Relative frequency")

#Let's subset the dfm to look at popular hastags
dfmat_hashtags <- dfm_select(dfmat_covidtweets, pattern = '#*') 
textplot_wordcloud(dfmat_hashtags, max_words = 100)
textplot_wordcloud(dfm_select(dfmat_covidtweets_gpd, pattern = '#*'), comparison = TRUE, max_words = 100, color = c("blue", "red"))
```

## Sentiment analysis of the twitter data

##### *ðŸ“– go to slides*

The aim of sentiment analysis is to determine the polarity of a text (i.e., whether the emotions expressed in it are rather positive or negative). This is often done by word lists and by counting terms that were previously assigned to the categories *positive* or *negative* (sometimes a third category is included; *neutral,* and sometimes words can also be assigned a sentiment strength). In some lexicons, words may also be assigned to emotions like joy, anger, sadness, and so forth.

So basically a dictionary ('sentiment/topic dictionary') is used to group a number of individual terms into a 'sentiment' category, then the content of the whole text as the sum of the sentiment content of the individual words. (This isn't the only way to approach sentiment analysis, but it is an often-used approach). The quanteda package provides access to four general-purpose sentiment dictionaries (for English). All of these lexicons are based on unigrams (i.e., single words), and some bigrams.

```{r sentiment_analysis, message=F}
#Dictionary based sentiment analysis
lengths(data_dictionary_LSD2015)
print(data_dictionary_LSD2015, max_nval = 20)

#Can also make and use your own bespoke dictionary
my_dictionary <- dictionary(list(pos = c("happiness", "joy", "light"), neg = c("sadness", "anger", "darkness")))

#Set the polarity categories we wish to use
my_polarity <- list(pos = c("positive", "neg_negative"), neg = c("negative", "neg_positive"))
polarity(data_dictionary_LSD2015) <- my_polarity

#Get sentiment scores on our original corpus 
sentiment<-textstat_polarity(dfmat_covidtweets, dictionary = data_dictionary_LSD2015)
range(sentiment$sentiment)
head(sentiment)

#default function == log(\frac{pos}{neg})
#There are other functions available - or you can write your own and feed it in using the option 'fun = xxx'
#If you have very long documents - you may want to weight the sentiment values by the length (total words) in the document. 

#--------------
#Let's see what is happening in the assignment
covid_toks[1:2]
tokens_lookup(covid_toks, data_dictionary_LSD2015, nested_scope = "dictionary", exclusive = FALSE)[1:2]
#--------------

#Alternative Method - use function tokens_lookup to assign sentiment to tokens
# --- covidtweets_lsd <- tokens_lookup(covid_toks, dictionary = data_dictionary_LSD2015)
# --- if you do the above - then need to create a document document-feature matrix
# --- dfmat_covidtweets_lsd <- dfm(covidtweets_lsd) %>% dfm_group(c('location_US_UK', 'week_day')) 
# --- head(dfmat_covidtweets_lsd)

#Create data for plotting
sentiment_dat<- cbind(sentiment, docvars(covid_corpus)) %>%
  filter(!is.na(location_US_UK)) %>% 
  mutate(week_day = fct_relevel(week_day, 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')) %>% 
  group_by(location_US_UK, week_day) %>% 
  summarise(mean_sent = mean(sentiment))
 
ggplot(data = sentiment_dat, aes(y = mean_sent, x = week_day, group = location_US_UK, color = location_US_UK))+
  geom_line()+
  geom_hline(yintercept = 0)+
  theme_bw()
```

## Other Dictionaries

```{r bing_dictionary}
#Sentiment "type" dictionary
lengths(data_dictionary_NRC)
print(data_dictionary_NRC, max_nval = 10)

#Alternative Method - use function tokens_lookup to assign sentiment to tokens
covidtweets_new_lsd <- tokens_lookup(covid_toks, dictionary = data_dictionary_NRC )
dfmat_sentiment_gpd <- dfm(covidtweets_new_lsd) %>% dfm_group(c('location_US_UK'))
dfm_sentiment_prop <- dfm_weight(dfmat_sentiment_gpd, scheme = "prop")

plot_dat <- dfm_sentiment_prop %>% 
  convert(., to ="data.frame") %>% 
  pivot_longer(!doc_id, names_to = "sentiment_types", values_to = "proportions")

ggplot(data = plot_dat, aes(x=sentiment_types, y=proportions, fill=doc_id)) + 
  geom_bar(stat="identity", width=0.7, position=position_dodge(width=0.8)) +
  theme_bw()+
  theme(legend.title = element_blank())+
  theme(axis.title.x = element_blank())
```

## Case Study: Using supervised machine learning to build a text classifier

##### *ðŸ“– go to slides*

```{r load_extra_packages}
#load the text mining, ML classification packages
library(quanteda.textmodels)
library(caret)
```

```{r prep_for_ML}
#*************************************************************
# Read in data for building the ML model ====
coded_freetext<-read_tsv(here('Working_Data/Supervised_ML/labelled_dataset.txt'))
head(coded_freetext)
dim(coded_freetext)

(labels_meanings<- coded_freetext %>%  
            select(label, meaning) %>% 
            mutate(label = as.character(label)) %>%  
            distinct() %>% 
            arrange(label))

#*************************************************************
# Create the corpus ====
freetext_corpus<-corpus(coded_freetext, text_field = 'freetext_clean')
#see bits of the corpus:
texts(freetext_corpus)[9:11]

#list the docvars [Label is a docvars]
head(docvars(freetext_corpus) )

#*************************************************************
# Build the Document Feature Matrix ====
dfm <- dfm(freetext_corpus, verbose = T, remove_punct = T, remove_numbers = T, remove_symbols = T, stem = T,
                           remove = stopwords('english')) 
dfm

#remove low freq words and terms only found in a few documents
dfm_nolowfreqs <-dfm_trim(dfm, min_termfreq = 5, min_docfreq = 3)
dfm_nolowfreqs
  dim(dfm_nolowfreqs)
  topfeatures(dfm_nolowfreqs, 20)
  head(featnames(dfm_nolowfreqs), 20) # first 50 features (in original text order)
  head(sort(featnames(dfm_nolowfreqs)), 20) # first 50 features alphabetically
  sparsity(dfm_nolowfreqs) 
```

**quanteda.textmodels** package includes models and classifiers for sparse matrix objects representing textual data in the form of a document-feature matrix. Includes methods for correspondence analysis, latent semantic analysis, and fast Naive Bayes and linear 'SVMs' specially designed for sparse textual data.

```{r build_SVMmodels}
#*************************************************************
#Split the data into training and testing datasets ==== A 60% that will become our training data set / 40% for our testing data set 
#set the seed so random draws are reproducible
set.seed(4321)
training_data <- dfm_sample(dfm_nolowfreqs, size = 0.6*nrow(dfm_nolowfreqs))
testing_data<-dfm[setdiff(docnames(dfm_nolowfreqs), docnames(training_data)), ]

#use base r if you want [[sample(1:nrow(dfm_nolowfreqs), floor(.60 * nrow(dfm_nolowfreqs)))]]

#run SVM classifier model on training data ====
svm_model <- textmodel_svm(training_data, training_data$label, type = 0)
svm_model <- textmodel_svm(training_data, training_data$label)
summary(svm_model)

#Get the predictions (force = T means to only use features that exist in the both training and prediction dfms)
svm_predicted_class <- predict(svm_model, newdata = testing_data, type = "class", force = T)
head(svm_predicted_class)
svm_predicted_prob <- predict(svm_model, newdata = testing_data, type = "probability", force = T)
head(svm_predicted_prob)
svm_actual_class <- testing_data$label
svm_tab_class <- table(svm_actual_class, svm_predicted_class); svm_tab_class
sum(diag(svm_tab_class))/dim(testing_data)[1]

#Look at accuracy etc (from caret package)
confusionMatrix(svm_tab_class, mode = "everything")
```

```{r apply_SVMclassifier}
#*************************************************************
#NOTES -->
  #Initial investigation showed that 60:40 was the best split (no improvement with higher % for training)
  #Initial investigation showed that stemming words did not improve accuracy or kappa significantly
  #Initial investigation showed that weighting the dfm (i.e. using tf-idf matrix) did not improve accuracy or kappa significantly
  #Initial investigation showed that svm model outperforms the nb model
  #If you need to do stemming then run this line of code
      #>  stemd_dfm <- dfm(freetext_corpus, verbose = T, remove_punct = T, remove_numbers = T, remove_symbols = T, remove = c(stopwords('english'), mystopwords), stem = T) 
  #If you want to use tf-idf weighting then run these lines of code
      #> base_dfm_nolows <-dfm_trim(base_dfm, min_termfreq = 6, min_docfreq = 3)
      #> based_dfm_tfidf <-dfm_tfidf(base_dfm_nolows, scheme_tf = "count", scheme_df = "inverse")

#***********************************************************************
# APPLY SVM model to predict classes of new freetext data fields ====

#load the new data that needs labelling
newfields_data<-read_tsv(here('Working_Data/Supervised_ML/unlabelled_dataset.txt'))
head(newfields_data)

#create the corpus
newfields_corpus<-corpus(newfields_data, text_field = 'freetext_clean') 
newdata_dfm <- dfm(newfields_corpus, verbose = T, remove_punct = T, remove_numbers = T, remove_symbols = T, stem = T, 
                            remove = c(stopwords('english'))) %>% 
               dfm_trim(min_termfreq = 5, min_docfreq = 3)

#Predict new classes for the new fields
newfields_predicted_label <- predict(svm_model, newdata = newdata_dfm, type = "class")

#bind the classifications to the original data
model_classified_newfields<-cbind(newfields_data, newfields_predicted_label) %>% 
          mutate(label = as.character(newfields_predicted_label)) %>% 
          left_join(labels_meanings, by = 'label') %>% 
          select(-doc_id, -freetext_field_id, -newfields_predicted_label)
model_classified_newfields
```

```{r spell_check_textfields}
#*************************************************************
library(hunspell)

# Check individual words
words <- c("beer", "wiskey", "wyne")
correct <- hunspell_check(words)
print(correct)
hunspell_suggest(words)
hunspell_suggest(words[!correct])

text <- pdftools::pdf_text(here('Working_Data/import_data_types/Morris_Lancet_2021.pdf'))
bad_words <- hunspell(text)
sort(unique(unlist(bad_words)))
#Can install a medical dictionary:
#See details in this blog post;
#https://science.data.blog/2018/11/23/correcting-misspelled-medical-words-in-raw-text-data-with-r/
#and here --> https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html#Custom_Dictionaries

#Automate the spell check process - a simple example:

text_data<-'Too Atlantic bottle-nosed dolfins have been traned by the Navy to serch waters for explosives nere the citty of London'

# Get bad words.
  incorrectly_spelled_words <- hunspell(text_data) %>% unlist()

# Extract the first suggestion for each bad word. - Is slow for large numbers of documents
  suggestions <- sapply(incorrectly_spelled_words, function(x) hunspell_suggest(x)[[1]][1]) %>% unname()

#replace in the original text - TAKES ~ A FEW  MINUTES TO RUN
  corrected_text <- stringi::stri_replace_all_regex(str = text_data, 
                          pattern = paste0('\\b', incorrectly_spelled_words, '\\b'),
                          replacement = suggestions, vectorize_all = FALSE)
  corrected_text

```
