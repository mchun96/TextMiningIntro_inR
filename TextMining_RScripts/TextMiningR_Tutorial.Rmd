---
title: "A brief introduction to text mining in R"
author: "Shelly Lachish"
date: "02/02/2021"
output: 
  html_document: 
    fig_width: 10
    fig_height: 8
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
library(data.table)
library(tidyverse)
library(here)
library(readtext)
library(quanteda)
library(lubridate)
```

## Importing text data: package(readtext)

The **readtext** function from the package with the same name, detects the file formats of a given list of files and extracts the content into a data.frame. **readtext** supports .txt, .json, .csv, .tab, .tsv, .xml, .pdf, .doc, .docx, .odt, .rtf, files from URLs and archive file (.zip, .tar, .tar.gz, .tar.bz). It can read in multiple files at once and supports glob/wildcard expressions.

The parameter *docvarsfrom* allows you to set metadata variables by splitting on file name or path names.

```{r importing_text_data, include=TRUE, warning=FALSE, message=FALSE}
#Import text data files
#..with docvars taken from file paths (file paths must be same length)
extracted_texts<- readtext(here('Working_Data/import_data_types/*'), docvarsfrom = "filepaths", dvsep = "/")
print(extracted_texts)
print(extracted_texts[1], n = 12)

#Docvars from file names (File names need to be standardised across list - same num/type separator)
extracted_texts <- readtext(here('Working_Data/import_data_types/Mindy*'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "place", "year"))
print(extracted_texts, n=4)

# View beginning of the first extracted text
cat(substr(extracted_texts$text[1] , 0, 400))

#Get pdfs
RC_data <- readtext(here('Working_Data/import_data_types/*.pdf'), docvarsfrom = "filenames", dvsep = "_", docvarnames = c("author", "journal", "year"))
RC_data
str(RC_data)
```

### A note on encodings

This is from the readtext package tutorial: "As encoding can also be a challenging issue for those reading in texts, we include functions for diagnosing encodings on a file-by-file basis, and allow you to specify vectorized input encodings to read in file types with individually set (and different) encodings. (All encoding functions are handled by the stringi package.)"

## From data to corpus/corpora

Here we will use a data set of \~7,000 tweets with the subject "covid19" (obtained using the rtweet package from the 8th - 15th Feb) to understand concepts in text analysis, do some basic text exploration, run some simple statistics and perform a sentiment analysis on the content of the tweets

```{r load_tweet_data, message = FALSE}

#Load the data
covid_data<-read_tsv(here('Working_Data/covid19_tweets.tsv'))

#Clean the dataset
covid_data_clean <- covid_data %>% 
  #Remove @mentions
  mutate(text_clean = str_replace_all(text, '@\\S+', '')) %>%
  #Remove URLs
  mutate(text_clean = str_replace_all(text_clean, 'http\\S+', '')) %>%  
  #trim new line indicators and retweet indicators
  mutate(text_clean = str_replace_all(text_clean, '\\n|\\r', '')) %>%
  #Replace ampersand indicators
  mutate(text_clean = str_replace_all(text_clean, '&amp', 'and')) %>%
  #Remove unicode character indicators <U+0001F449> (used in place of emojis)
  mutate(text_clean = str_replace_all(text_clean, '<U.*>', '')) %>%
  #trim leading/tailing whitespace
  mutate(text_clean = trimws(text_clean, which = 'both')) %>%     
  #Format date to ymd
  mutate(tweet_date = as.Date(lubridate::ymd_hms(created_at))) %>% 
  #Add day of week variable
  mutate(week_day = lubridate::wday(ymd_hms(created_at), label = T)) %>%
  #Add UK vs US location variable
  mutate(location_US_UK = ifelse(str_detect(location, regex("uk|britain|england|scotland|wales", ignore_case = T)), "UK", 
                                 ifelse(str_detect(location, regex("us|usa|america*", ignore_case = T)), "US", NA))) %>%
  select (-text, -created_at)
```

```{r build_corpus}

#Create your corpus
covid_corpus<-corpus(covid_data_clean, text_field = 'text_clean')
head(docvars(covid_corpus))

#Inspecting the corpus
texts(covid_corpus)[2]
covid_data$text[2]
summary(covid_corpus, n = 50)

#Can save and plot from this (but only summary of first 50 texts)
corp_summary <- summary(covid_corpus)
ggplot(data = corp_summary, aes(x = log(followers_count), y = Tokens, group = 1)) + geom_line() + geom_point()  + theme_bw()

```

The corpus can be manipulated in a variety of ways (somewhat analogous to a data frame). You can subset, concatenate, trim components from the text body, and change the unit of texts between documents, paragraphs and sentences. You can extract segments of texts and tags from documents (useful when you analyze sections of documents or transcripts separately). You can also perform a keywords-in-context search (kwic function).

```{r corpus_manipulations}
#Corpus manipulations
#Subset a corpus
corp_monday <- corpus_subset(covid_corpus, week_day == 'Mon')
ndoc(corp_monday)

#concatenate two corpus
corp_tuesday <- corpus_subset(covid_corpus, week_day == 'Tue')
ndoc(corp_tuesday)
new_corp <- corpus(corp_tuesday + corp_monday)
ndoc(new_corp)

#change unit length to sentences/paragraphs/ or document (default)
corp_sentences <- corpus_reshape(corp_monday, to = "sentences")
ndoc(corp_sentences)

#The kwic function (keywords-in-context) performs a search for a word and allows us to view the contexts in which it occurs:
head(kwic(covid_corpus, pattern = 'vaccin*'))
head(kwic(covid_corpus, pattern = phrase("conspiracy theories")))
texts(covid_corpus)[3138]

```

## Tokenisation

The function `tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries. Usually a corpus is passed to `tokens()`, but it works with a character string too. By default, `tokens()` only removes separators (typically white spaces), but you can set other separators. You can (should?) remove punctuation and numbers prior to tokenisation.

You can remove tokens that you are not interested in using `tokens_select()`. Usually we remove grammatical words that have little or no substantive meaning in pre-processing. These are known as **stopwords**. The function `stopwords()` returns a pre-defined list of function words.

```{r get_tokens}
#Get Tokens

#Describe a bit how tokens are interpreted
txt <- c(text1 = 'This is $10 in 999 different ways,\n up and down; left and #right!', 
         text2 = '@shellstar working: @ something on #Rtexttutorial 2day\t4ever !')#
tokens(txt)
tokens(txt, remove_punct = TRUE, remove_symbols = T, remove_numbers = T)

#----------------------------------------
#Get tokens from covid_tweets data
#----------------------------------------
#Remove punctuation, symbols, numbers
head(covid_toks<-tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T))

#Remove stopwords
head(covid_toks_nostop <- tokens_remove(covid_toks, pattern = stopwords("en")))

#Select only interesting words
head(covid_toks_vaccination <- tokens_select(covid_toks, pattern = c("vaccin*", "immunis*")))
```

To preserve [multiword expressions]{.ul} (which you may want to do in a bag-of-word type analysis), you have to compound them using `tokens_compound()`. For example, `tokens_compound(covid_toks, pattern = phrase(c('Boris Johnson','hospital admission*')))`. You can discover multiword expressions in your tokens using `textstat_collocations()`. This kind of thing is also useful if you want to create negative bigrams: `toks_neg_bigram<-tokens_compound(toks, pattern = phrase("not *"))` and then select them from your corpus : `toks_neg_bigram_select <- tokens_select(toks_neg_bigram, pattern = phrase("not_*"))`

## From tokens to dfm (document feature matrix, aka document term matrix)

The function `dfm()` constructs a document-feature matrix (DFM) from a tokens object or a corpus object (in which case the tokenisation occurs internally). `topfeatures()` will list the features (terms/tokens) in descending order. `textstat_frequency()` shows both term and document frequencies. You can also use the function to find the most frequent features within groups.

```{r get_dfm}
covid_toks <- tokens(covid_corpus, remove_punct = TRUE, remove_symbols = T, remove_numbers = T)

my_stopwords<-c('covid', 'covid19', 'covid-19', 'coronavirus', '#covid', '#covid19', '#covid-19', '#coronavirus')

dfmat_covidtweets <- dfm(covid_toks, tolower = TRUE, stem = FALSE, remove = c(stopwords(), my_stopwords))

dfmat_covidtweets
  ndoc(dfmat_covidtweets)
  nfeat(dfmat_covidtweets)
  head(docnames(dfmat_covidtweets))
  head(featnames(dfmat_covidtweets))
  head(rowSums(dfmat_covidtweets), 10)
  head(colSums(dfmat_covidtweets), 10)

topfeatures(dfmat_covidtweets, 10)

#Can be a good idea to remove very sparse terms from the dfm
(dfmat_covidtweets<-dfmat_covidtweets %>% dfm_trim(min_termfreq = 5, verbose = FALSE))

```

## Simple text analysis of the tweets

```{r text_exploration}

tstat_freq <- textstat_frequency(dfmat_covidtweets)
head(tstat_freq, 5)
tail(tstat_freq, 5)

dfmat_covidtweets %>% 
  textstat_frequency(n = 30) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

#Look at grouping variables  
dfmat_covidtweets_gpd <- dfm_group(dfmat_covidtweets, groups = 'location_US_UK')
tstat_freq_gpd <- textstat_frequency(dfmat_covidtweets, n = 30, groups = 'location_US_UK')

#Plot group differences
ggplot(data = tstat_freq_gpd, aes(x = factor(nrow(tstat_freq_gpd):1), y = frequency)) +
    geom_point() +
    facet_wrap(~ group, scales = "free") +
    coord_flip() +
    scale_x_discrete(breaks = nrow(tstat_freq_gpd):1,
                       labels = tstat_freq_gpd$feature) +
    labs(x = NULL, y = "Relative frequency")


#Let's take a look at a word cloud
set.seed(132)
textplot_wordcloud(dfmat_covidtweets, max_words = 100)
textplot_wordcloud(dfmat_covidtweets_gpd, comparison = TRUE, max_words = 100, color = c("blue", "red"))

##SHOULD WE STEM TERMS? - we have 'vaccine' and 'vaccines' and vaccination as top terms
(dfmat_covidtweets_stem <- dfm(covid_toks, tolower = TRUE, stem = TRUE, remove = c(stopwords(), my_stopwords)))
tstat_freq_stem <- textstat_frequency(dfmat_covidtweets_stem, n = 30)
head(tstat_freq_stem, 20)
textplot_wordcloud(dfmat_covidtweets_stem, max_words = 100)

#Let's subset the dfm to look at popular hastags
dfmat_hashtags <- dfm_select(dfmat_covidtweets, pattern = '#*') 
textplot_wordcloud(dfmat_hashtags, max_words = 100)
```

As with `tokens()`, you can select features from a DFM using `dfm_select()`. You can also select features based on the length of features (e.g. keep features consisting of at least five characters: `dfm_keep(dfmat_inaug, min_nchar = 5)`). While `dfm_select()` selects features based on patterns, `dfm_trim()` does this based on feature frequencies. If `min_termfreq = 10`, features that occur less than 10 times in the corpus are removed.

If you want to convert the frequency count to a proportion within documents, use `dfm_weight(scheme = "prop")`

## sentiment analysis of twitter data
